[Here](https://www.youtube.com/watch?v=Ofk7G3GD9jk) is a good public lecture to explain **RDD**, **DataFrames** and **datasets** in Spark. Some of the highlights are:
-  **RDD** is a low-level API, which provides logical model to process data. And it is somewhat *lazy*.
-  **DataFrames** is a high-level API and offers some merits: Faster than **RDD**, using less memory and implementing some code optimization.
